{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c96576d",
   "metadata": {},
   "source": [
    "# Assemble Model using DenseNet, EfficientNet, ResNet50, XGBoost, Light GBM, and CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0552969-dff4-4a3e-9bb9-361925f9e4c4",
   "metadata": {},
   "source": [
    "## Here's datasets you will need\n",
    "- train-image.hdf5\n",
    "- train-metadata.csv\n",
    "- augmented_data.hdf5\n",
    "- augmented_metadata.csv\n",
    "- isic_image.hdf5\n",
    "- isic_metadata.csv\n",
    "- test-image.hdf5\n",
    "- test-metadata.csv\n",
    "## Here's the models/paths you need to run without training the CNN model\n",
    "- DenseNet121_checkpoints/DenseNet121_epoch_20.pth\n",
    "- EfficientNet-B3_checkpoints/EfficientNet-B3_epoch_20.pth\n",
    "- ResNet50_checkpoints/ResNet50_epoch_20.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb04e90",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32cc9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    __slots__ = ['images', 'labels', 'augment', 'transform']\n",
    "\n",
    "    def __init__(self, images, labels, augment=False, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert from numpy to PIL only if needed\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.dtype != np.uint8:\n",
    "                image = (image * 255).clip(0, 255).astype(np.uint8)  # ensure valid range\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Directly return float tensor\n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65dd2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "# from dataset import HDF5Dataset\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from isic_metric import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d49bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863765d",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0a64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "metadata = []\n",
    "malignant_count = 0\n",
    "benign_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a898a1d",
   "metadata": {},
   "source": [
    "### First, load all data from original database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547e19de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load isic_ids from 3 csv files\n",
    "test_ids = pd.read_csv(\"test_id.csv\")[\"isic_id\"].tolist()\n",
    "train_ids = pd.read_csv(\"train_id.csv\")[\"isic_id\"].tolist()\n",
    "val_ids = pd.read_csv(\"val_id.csv\")[\"isic_id\"].tolist()\n",
    "# only load 600 for training, 150 for validation, 250 for testing\n",
    "# train_ids = train_ids[:600]\n",
    "# val_ids = val_ids[:150]\n",
    "# test_ids = test_ids[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07d5d2",
   "metadata": {},
   "source": [
    "## read in each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7f05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_hdf5_path = 'train-image.hdf5'\n",
    "original_train_metadata_path = 'train-metadata.csv'\n",
    "original_train_metadata = pd.read_csv(original_train_metadata_path,low_memory=False)   \n",
    "original_train_metadata.set_index('isic_id', inplace=True)\n",
    "original_train_hdf5 = h5py.File(original_train_hdf5_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a5d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "metadata_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "metadata_val = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "metadata_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1211ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(hdf5_file, metadata_df, id_list):\n",
    "    images = []\n",
    "    labels = []\n",
    "    meta_records = []\n",
    "\n",
    "    for isic_id in tqdm(id_list):\n",
    "        # load and decode image\n",
    "        image = hdf5_file[isic_id][()]\n",
    "        image = np.frombuffer(image, dtype=np.uint8)\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (128, 128))\n",
    "        image = image / 255.0  # normalize\n",
    "        \n",
    "        # get label and metadata\n",
    "        record = metadata_df.loc[isic_id]\n",
    "        label = int(record[\"target\"])\n",
    "        \n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        meta_records.append(record)\n",
    "\n",
    "    return images, labels, meta_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a80fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240635/240635 [02:47<00:00, 1435.29it/s]\n",
      "100%|██████████| 60159/60159 [00:55<00:00, 1081.35it/s]\n",
      "100%|██████████| 100265/100265 [01:35<00:00, 1047.11it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, metadata_train = load_images_and_labels(original_train_hdf5, original_train_metadata, train_ids)\n",
    "X_val, y_val, metadata_val = load_images_and_labels(original_train_hdf5, original_train_metadata, val_ids)\n",
    "X_test, y_test, metadata_test = load_images_and_labels(original_train_hdf5, original_train_metadata, test_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3372fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malignant count: 236\n",
      "Benign count: 240399\n"
     ]
    }
   ],
   "source": [
    "malignant_count = sum(y == 1 for y in y_train)\n",
    "benign_count = sum(y == 0 for y in y_train)\n",
    "print(f\"Malignant count: {malignant_count}\")\n",
    "print(f\"Benign count: {benign_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "385d384e-0605-4008-a2c6-988c0c2ec895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 240635\n",
      "Validation data: 60159\n",
      "Test data: 100265\n",
      "Metadata Training data: 240635\n",
      "Metadata Validation data: 60159\n",
      "Metadata Test data: 100265\n",
      "Malignant count: 236\n",
      "Benign count: 240399\n"
     ]
    }
   ],
   "source": [
    "print(f'Training data: {len(X_train)}')\n",
    "print(f'Validation data: {len(X_val)}')\n",
    "print(f'Test data: {len(X_test)}')\n",
    "print(f'Metadata Training data: {len(metadata_train)}')\n",
    "print(f'Metadata Validation data: {len(metadata_val)}')\n",
    "print(f'Metadata Test data: {len(metadata_test)}')\n",
    "print(f'Malignant count: {malignant_count}')\n",
    "print(f'Benign count: {benign_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78d322",
   "metadata": {},
   "source": [
    "## Load CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c34365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b756755",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35044d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from isic_metric import score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, device, train_dataset, val_dataset, nn_name, weights, transform, model, num_pos, num_neg, lr=1e-5, num_epochs=20):\n",
    "        self.device = device\n",
    "        self.weights = weights\n",
    "        self.transform = transform\n",
    "        self.model = model\n",
    "\n",
    "        if nn_name == \"EfficientNet-B3\":\n",
    "            print(\"EfficientNet Configuration\")\n",
    "            in_features = self.model.classifier[1].in_features\n",
    "            self.model.classifier[1] = nn.Linear(in_features, 1)\n",
    "        elif nn_name == \"DenseNet121\":\n",
    "            in_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(in_features, 1)\n",
    "        elif nn_name == \"ResNet50\":\n",
    "            model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # Pos weight to handle imbalance\n",
    "        pos_weight = torch.tensor([num_neg / num_pos], dtype=torch.float).to(self.device)\n",
    "        print(f'Using pos_weight: {pos_weight}')\n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()  # AMP scaler\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.val_precisions = []\n",
    "        self.val_recalls = []\n",
    "        self.val_f1s = []\n",
    "        self.partial_aucs = []\n",
    "\n",
    "        # Optimized DataLoaders\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=64, shuffle=True,\n",
    "            num_workers=0, pin_memory=True, persistent_workers=False\n",
    "        )\n",
    "        self.val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=64, shuffle=False,\n",
    "            num_workers=0, pin_memory=True, persistent_workers=False\n",
    "        )\n",
    "\n",
    "        self.nn_name = nn_name\n",
    "        if not os.path.exists(f\"{self.nn_name}_checkpoints\"):\n",
    "            os.makedirs(f\"{self.nn_name}_checkpoints\")\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for inputs, labels in tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.num_epochs}\"):\n",
    "                inputs = inputs.to(self.device, non_blocking=True)\n",
    "                labels = labels.float().unsqueeze(1).to(self.device, non_blocking=True)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                with torch.autocast(device_type='cuda'):\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            self.train_losses.append(epoch_loss / len(self.train_loader))\n",
    "\n",
    "            # === Validation ===\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            preds, targets = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in tqdm(self.val_loader, desc=f\"Validation\"):\n",
    "                    inputs = inputs.to(self.device, non_blocking=True)\n",
    "                    labels = labels.float().unsqueeze(1).to(self.device, non_blocking=True)\n",
    "\n",
    "                    with torch.autocast(device_type='cuda'):\n",
    "                        outputs = self.model(inputs)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "                    targets.extend(labels.cpu().numpy())\n",
    "\n",
    "            self.val_losses.append(val_loss / len(self.val_loader))\n",
    "\n",
    "            # Threshold predictions at 0.5\n",
    "            preds = np.array(preds).flatten()\n",
    "            true_labels = np.array(targets).astype(int).flatten()\n",
    "            pred_labels = (preds >= 0.5).astype(int)\n",
    "\n",
    "            # Compute pAUC\n",
    "            df_sol = pd.DataFrame({\"image_id\": list(range(len(true_labels))), \"target\": true_labels})\n",
    "            df_sub = pd.DataFrame({\"image_id\": list(range(len(preds))), \"target\": preds})\n",
    "            pauc = score(df_sol, df_sub, row_id_column_name=\"image_id\", min_tpr=0.80)\n",
    "            self.partial_aucs.append(pauc)\n",
    "\n",
    "            val_acc = accuracy_score(true_labels, pred_labels)\n",
    "            val_precision = precision_score(true_labels, pred_labels)\n",
    "            val_recall = recall_score(true_labels, pred_labels)\n",
    "            val_f1 = f1_score(true_labels, pred_labels)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Acc = {val_acc:.4f}, Precision = {val_precision:.4f}, Recall = {val_recall:.4f}, F1 = {val_f1:.4f}, pAUC = {pauc:.4f}\")\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            self.val_precisions.append(val_precision)\n",
    "            self.val_recalls.append(val_recall)\n",
    "            self.val_f1s.append(val_f1)\n",
    "\n",
    "        # === Save final checkpoint ===\n",
    "        torch.save({\n",
    "            'epoch': self.num_epochs,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_loss': self.train_losses[-1],\n",
    "            'val_loss': self.val_losses[-1],\n",
    "            'val_auc': self.partial_aucs[-1]\n",
    "        }, f\"{self.nn_name}_checkpoints/{self.nn_name}_final.pth\")\n",
    "\n",
    "        # === Plotting ===\n",
    "        plt.plot(self.train_losses, label='Train')\n",
    "        plt.plot(self.val_losses, label='Validation')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"{self.nn_name} Training Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{self.nn_name}_loss_curve.png\")\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(list(range(1, self.num_epochs + 1)), self.partial_aucs, marker='o', label=\"Partial AUC @ TPR>0.8\")\n",
    "        plt.title(f\"{self.nn_name} Partial AUC over Epochs (FPR ≤ 0.2)\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Partial AUC (scaled)\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.nn_name}_partial_auc_curve.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcc2dff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pos_weight: tensor([1018.6398], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mperform\\AppData\\Local\\Temp\\1\\ipykernel_4980\\2245608217.py:39: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()  # AMP scaler\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "# from ModelTrainer import Trainer\n",
    "densenet_weights = DenseNet121_Weights.DEFAULT\n",
    "densenet_transform = densenet_weights.transforms()\n",
    "densenet_train_dataset = HDF5Dataset(X_train, y_train, augment=True, transform=densenet_transform)\n",
    "densenet_val_dataset = HDF5Dataset(X_val, y_val, augment=False, transform=densenet_transform)\n",
    "densenet_model = densenet121(weights=densenet_weights)\n",
    "lr = 1e-5\n",
    "num_epochs = 20\n",
    "dense_net_trainer = Trainer(device, densenet_train_dataset, densenet_val_dataset, \"DenseNet121\", densenet_weights, densenet_transform, densenet_model, malignant_count, benign_count, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c247174f-cfff-4084-b71f-f95c879a1818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   2%|▏         | 60/3760 [00:38<39:08,  1.58it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdense_net_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()\n\u001b[32m     67\u001b[39m epoch_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github repos\\EECS-545-final-project\\.env\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github repos\\EECS-545-final-project\\.env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github repos\\EECS-545-final-project\\.env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github repos\\EECS-545-final-project\\.env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mHDF5Dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     38\u001b[39m     image = Image.fromarray(image)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Directly return float tensor\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, torch.tensor(label, dtype=torch.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github repos\\EECS-545-final-project\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github repos\\EECS-545-final-project\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github repos\\EECS-545-final-project\\.env\\Lib\\site-packages\\torchvision\\transforms\\_presets.py:62\u001b[39m, in \u001b[36mImageClassification.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, Tensor):\n\u001b[32m     61\u001b[39m     img = F.pil_to_tensor(img)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m img = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_image_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m img = F.normalize(img, mean=\u001b[38;5;28mself\u001b[39m.mean, std=\u001b[38;5;28mself\u001b[39m.std)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github repos\\EECS-545-final-project\\.env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:243\u001b[39m, in \u001b[36mconvert_image_dtype\u001b[39m\u001b[34m(image, dtype)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, torch.Tensor):\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput img should be Tensor Image\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_image_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github repos\\EECS-545-final-project\\.env\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:98\u001b[39m, in \u001b[36mconvert_image_dtype\u001b[39m\u001b[34m(image, dtype)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# int to float\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# TODO: replace with dtype.is_floating_point when torchscript supports it\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.tensor(\u001b[32m0\u001b[39m, dtype=dtype).is_floating_point():\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     image = \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image / input_max\n\u001b[32m    101\u001b[39m output_max = \u001b[38;5;28mfloat\u001b[39m(_max_value(dtype))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dense_net_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea77a71",
   "metadata": {},
   "source": [
    "### Load model if already trained and calculate pAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# densenet_model_path = \"DenseNet121_checkpoints/DenseNet121_epoch_20.pth\"\n",
    "# densenet_checkpoint = torch.load(densenet_model_path, weights_only=False, map_location=device)\n",
    "# dense_net_trainer.model.load_state_dict(densenet_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f6b59",
   "metadata": {},
   "source": [
    "### Calculate test pAUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calc_pauc import pAUC\n",
    "dense_net_trainer.model.eval()\n",
    "calc_pAUC = pAUC(device, dense_net_trainer.model, dense_net_trainer.transform, X_test, y_test,  metadata_test)\n",
    "pAUC_val = calc_pAUC.compute_pAUC()\n",
    "print(f\"pAUC for densenet: {pAUC_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe25088-416d-456f-b84d-77a9ca36d32c",
   "metadata": {},
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914910f-00ef-48d5-b255-de089719c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "efficientnet_weights = EfficientNet_B3_Weights.DEFAULT\n",
    "efficientnet_transform = efficientnet_weights.transforms()\n",
    "efficientnet_train_dataset = HDF5Dataset(X_train, y_train, augment=True, transform=efficientnet_transform)\n",
    "efficientnet_val_dataset = HDF5Dataset(X_val, y_val, augment=False, transform=efficientnet_transform)\n",
    "efficientnet_model = efficientnet_b3(weights=efficientnet_weights)\n",
    "lr = 3e-5\n",
    "num_epochs = 20\n",
    "efficientnet_trainer = Trainer(device, efficientnet_train_dataset, efficientnet_val_dataset, \"EfficientNet-B3\", efficientnet_weights, efficientnet_transform, efficientnet_model, malignant_count, benign_count, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1ab4b",
   "metadata": {},
   "source": [
    "### Load EfficientNet Model if already trained and compute pAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad971031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficientnet_model_path = \"EfficientNet-B3_checkpoints/EfficientNet-B3_epoch_18.pth\"\n",
    "# efficientnet_checkpoint = torch.load(efficientnet_model_path, weights_only=False, map_location=device)\n",
    "# efficientnet_model.load_state_dict(efficientnet_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac052bd",
   "metadata": {},
   "source": [
    "### pAUC calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ecc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_model.eval()\n",
    "calc_pAUC = pAUC(device, efficientnet_trainer.model, efficientnet_trainer.transform, X_test, y_test,  metadata_test)\n",
    "pAUC_val = calc_pAUC.compute_pAUC()\n",
    "print(f\"pAUC for efficientnet: {pAUC_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb181cd5",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5018a24-9c63-474d-9db9-eb72ac9b43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "resnet_weights = ResNet50_Weights.DEFAULT\n",
    "resnet_transform = resnet_weights.transforms()\n",
    "\n",
    "resnet_train_dataset = HDF5Dataset(X_train, y_train, transform=resnet_transform)\n",
    "resnet_val_dataset = HDF5Dataset(X_val, y_val, transform=resnet_transform)\n",
    "resnet_model = resnet50(pretrained=True)\n",
    "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 1)\n",
    "\n",
    "lr = 4e-5\n",
    "num_epochs = 20\n",
    "resnet_trainer = Trainer(device, resnet_train_dataset, resnet_val_dataset, \"ResNet50\", resnet_weights, resnet_transform, resnet_model, malignant_count, benign_count, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779821f",
   "metadata": {},
   "source": [
    "### Load ResNet50 if already trained and calculate pAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de725b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet_model_path = \"ResNet50_checkpoints/ResNet50_epoch_20.pth\"\n",
    "# resnet_checkpoint = torch.load(resnet_model_path, weights_only=False, map_location=device)\n",
    "# resnet_trainer.model.load_state_dict(resnet_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8cd26a",
   "metadata": {},
   "source": [
    "### pAUC Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.eval()\n",
    "calc_pAUC = pAUC(device, resnet_trainer.model, resnet_trainer.transform, X_test, y_test,  metadata_test)\n",
    "pAUC_val = calc_pAUC.compute_pAUC()\n",
    "print(f\"pAUC for resnet: {pAUC_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e9a13",
   "metadata": {},
   "source": [
    "## Output predictions for train/val/test for all CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75e45a-208b-4927-89b3-aa0146aa1259",
   "metadata": {},
   "source": [
    "#### Convert CNNs's output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768adf28-0940-439f-88ef-52c3d0f4ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "class TorchCNNWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, device='cpu', transform=None, threshold=0.5):\n",
    "        self.model = model.eval().to(device)\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def _prepare_image(self, img):\n",
    "        if isinstance(img, np.ndarray):\n",
    "            if img.dtype != np.uint8:\n",
    "                img = (img * 255).astype(np.uint8)\n",
    "            img = Image.fromarray(img)\n",
    "        return self.transform(img).unsqueeze(0).to(self.device)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        probs = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for img in tqdm(X, desc=\"Predicting with CNN\"):\n",
    "                img_tensor = self._prepare_image(img)\n",
    "                logits = self.model(img_tensor)\n",
    "                prob = torch.sigmoid(logits).cpu().item()\n",
    "                probs.append(prob)\n",
    "                # probs.append([1 - prob, prob])\n",
    "    \n",
    "        return probs\n",
    "\n",
    "    # def predict(self, X):\n",
    "    #     probs = self.predict_proba(X)\n",
    "        \n",
    "    #     # return (probs[:, 1] >= self.threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291ad03d-de31-4855-aa3c-196cc044e568",
   "metadata": {},
   "source": [
    "### Convert model into essemble model compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4e945-cab7-47f0-97fe-4291ac49c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_wrapper = TorchCNNWrapper(\n",
    "    model=dense_net_trainer.model,\n",
    "    device=device,\n",
    "    transform=dense_net_trainer.transform\n",
    ")\n",
    "\n",
    "efficientnet_wrapper = TorchCNNWrapper(\n",
    "    model=efficientnet_trainer.model,\n",
    "    device=device,\n",
    "    transform=efficientnet_trainer.transform\n",
    ")\n",
    "\n",
    "resnet_wrapper = TorchCNNWrapper(\n",
    "    model=resnet_trainer.model,\n",
    "    device=device,\n",
    "    transform=resnet_trainer.transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4da3d-9613-4616-a622-f2f4ca4ee506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for training set\n",
    "densenet_train_preds = densenet_wrapper.predict_proba(X_train)\n",
    "efficientnet_train_preds = efficientnet_wrapper.predict_proba(X_train)\n",
    "resnet_train_preds = resnet_wrapper.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for validation set\n",
    "densenet_val_preds = densenet_wrapper.predict_proba(X_val)\n",
    "efficientnet_val_preds = efficientnet_wrapper.predict_proba(X_val)\n",
    "resnet_val_preds = resnet_wrapper.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dff358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for test set\n",
    "densenet_test_preds = densenet_wrapper.predict_proba(X_test)\n",
    "efficientnet_test_preds = efficientnet_wrapper.predict_proba(X_test)\n",
    "resnet_test_preds = resnet_wrapper.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train predictions\n",
    "train_preds = pd.concat([\n",
    "    pd.Series([row['isic_id'] for row in metadata_train], name=\"isic_id\"),\n",
    "    pd.Series(densenet_train_preds, name=\"densenet\"),\n",
    "    pd.Series(efficientnet_train_preds, name=\"efficientnet\"),\n",
    "    pd.Series(resnet_train_preds, name=\"resnet\"),\n",
    "    pd.Series(y_train, name=\"GroundTruth\")\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pd concat to combine the predictions\n",
    "val_preds = pd.concat([\n",
    "    pd.Series([row['isic_id'] for row in metadata_val], name=\"isic_id\"),\n",
    "    pd.Series(densenet_val_preds, name=\"densenet\"),\n",
    "    pd.Series(efficientnet_val_preds, name=\"efficientnet\"),\n",
    "    pd.Series(resnet_val_preds, name=\"resnet\"),\n",
    "    pd.Series(y_val, name=\"GroundTruth\")\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pd concat to combine the predictions\n",
    "test_preds = pd.concat([\n",
    "    pd.Series([row['isic_id'] for row in metadata_test], name=\"isic_id\"),\n",
    "    pd.Series(densenet_test_preds, name=\"densenet\"),\n",
    "    pd.Series(efficientnet_test_preds, name=\"efficientnet\"),\n",
    "    pd.Series(resnet_test_preds, name=\"resnet\"),\n",
    "    pd.Series(y_test, name=\"GroundTruth\")\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50455eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.to_csv(\"train_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions to csv\n",
    "val_preds.to_csv(\"val_predictions.csv\", index=False)\n",
    "test_preds.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489439a3",
   "metadata": {},
   "source": [
    "## Load Tree based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1870b68",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f1315-44d5-45a4-b420-989fbd8e911a",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4407d2-c9ad-470a-bbbf-ee5db2469f8c",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
